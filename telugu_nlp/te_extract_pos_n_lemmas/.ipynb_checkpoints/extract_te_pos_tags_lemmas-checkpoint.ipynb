{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting POS Tags and Lemmatize the words - Telugu\n",
    "\n",
    "**Notebook Author:** Nirupam Purushothama\n",
    "\n",
    "1. Using the [library code](https://bitbucket.org/sivareddyg/telugu-part-of-speech-tagger/src/master/) of IIIT-LTRC research team to extract the POS Tags and Lemmas of words \n",
    "2. That library code is not updated to work with Python3.xx\n",
    "3. I modified the code to work with Python3.xx and also work as a library for my use\n",
    "\n",
    "I am checking in the code as part of my repository because my changes are temporary and will work with my code. For any latest updates you should refer to the IIIT-LTRC code for latest updates. The code here is just "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect\n",
    "import importlib\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "lib_dir = parent_dir + \"/lib/lang_tools_te/bin\"\n",
    "\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "    \n",
    "if lib_dir not in sys.path:\n",
    "    sys.path.insert(0, lib_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tag2vert' from 'C:\\\\Users\\\\LAVAN\\\\Desktop\\\\POS\\\\Nirupam\\\\telugu_nlp/lib/lang_tools_te/bin\\\\tag2vert.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unitok_lib as unlib\n",
    "importlib.reload(sys.modules['unitok_lib'])\n",
    "import te_helpers as tehp\n",
    "importlib.reload(sys.modules['te_helpers'])\n",
    "import lemmatiser as lmt\n",
    "importlib.reload(sys.modules['lemmatiser'])\n",
    "import tag2vert as tv\n",
    "importlib.reload(sys.modules['tag2vert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp file names\n",
    "words_tmp_file_name = \"telugu_words.tmp\"\n",
    "tagger_op_file = \"tagger_out.tmp\"\n",
    "\n",
    "# Tagger file path\n",
    "lang_tools_path = \"../lib/lang_tools_te\"\n",
    "tagger_file_path = lang_tools_path + \"/bin/tnt\"\n",
    "models_path = lang_tools_path + \"/models/telugu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The way these functions work are as follows:\n",
    "1. Convert input sentences to tokens using unitok.py\n",
    "2. Take these tokens and then get their POS tags using ./bin/tnt (unfortunately this has to be run like a script)\n",
    "3. Then using this output call lemmatizer and get the lemmatized words\n",
    "4. Then massage this data using tag2vert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tmp_file(te_list):\n",
    "    with open(words_tmp_file_name, 'w', encoding='utf-8') as file: \n",
    "        file.write(\"<doc>\\n\")\n",
    "        \n",
    "        for word in te_list:\n",
    "            file.write(word+\"\\n\")\n",
    "        \n",
    "        file.write(\"</doc>\")\n",
    "        \n",
    "    return\n",
    "\n",
    "def run_tagger_script():\n",
    "    os.system(tagger_file_path + \" -H -v0 \"+ models_path + \" \" + words_tmp_file_name + \n",
    "              \" | sed -e 's/\\t\\+/\\t/g' > \" + tagger_op_file)\n",
    "    \n",
    "    return\n",
    "\n",
    "def read_tagger_output():\n",
    "    texts_list = []\n",
    "    \n",
    "    with open(tagger_op_file, 'r', encoding='utf-8') as file:\n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            texts_list.append(line.strip())\n",
    "            \n",
    "        texts_list.pop()\n",
    "            \n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_string = \"అసలు పిల్లలను కనక ముందు ఇక్కడ కనాలా అక్కడ కనాలా అని బోలెడంత సంఘర్షణ. ఇప్పుడు అమెరికా పౌరసత్వం తీసుకొనే అవకాశం వస్తే తీసుకోవాలా వద్దా అనేది మరో పెద్ద సంఘర్షణ.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 0-3: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5820aded4f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mte_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLANGUAGE_DATA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'telugu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mte_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtehp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_non_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwrite_to_tmp_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrun_tagger_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mte_postags_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_tagger_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-58c56d6ca629>\u001b[0m in \u001b[0;36mwrite_to_tmp_file\u001b[1;34m(te_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mte_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</doc>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 0-3: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "te_list = unlib.tokenise(te_string, unlib.LANGUAGE_DATA['telugu']())\n",
    "te_list = tehp.remove_non_words(te_list)\n",
    "write_to_tmp_file(te_list)\n",
    "run_tagger_script()\n",
    "te_postags_list = read_tagger_output()\n",
    "lmt.load_lemmatiser_default(lang_tools_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "అసలు అసలు NN\n",
      "పిల్లలను పిల్ల NN\n",
      "కనక కనకం CC\n",
      "ముందు ముందు NST\n",
      "ఇక్కడ ఇక్కడ NST\n",
      "కనాలా కను VM\n",
      "అక్కడ అక్కడ NST\n",
      "కనాలా కను VM\n",
      "అని అని UT\n",
      "బోలెడంత బోలెడంత QF\n",
      "సంఘర్షణ సంఘర్షణ NN\n",
      ". None SYM.punc....\n",
      "ఇప్పుడు ఇప్పుడు NST\n",
      "అమెరికా అమెరికా NNP\n",
      "పౌరసత్వం పౌరసత్వం NN\n",
      "తీసుకొనే తీసుకో VM\n",
      "అవకాశం అవకాశం NN\n",
      "వస్తే వచ్చు VM\n",
      "తీసుకోవాలా తీసుకో VM\n",
      "వద్దా వద్దు VM\n",
      "అనేది అను UT\n",
      "మరో మరో QF\n",
      "పెద్ద పెద్ద JJ\n",
      "సంఘర్షణ సంఘర్షణ NN\n",
      ". None SYM.punc....\n"
     ]
    }
   ],
   "source": [
    "for item in te_postags_list:\n",
    "    word_tag = item.split('\\t')\n",
    "    lemma = lmt.lemmatise_word(word_tag[0], word_tag[1])\n",
    "    word, lemma_corr, pos_tag = tv.massage_lemma(word_tag[0], lemma, word_tag[1])\n",
    "    print(word, lemma_corr, pos_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
